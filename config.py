# MODE = 'django'
#
# SOURCE_VOCAB_SIZE = 2490 # 2492 # 5980
# TARGET_VOCAB_SIZE = 2101 # 2110 # 4830 #
# RULE_NUM = 222 # 228
# NODE_NUM = 96
#
# NODE_EMBED_DIM = 256
# EMBED_DIM = 128
# RULE_EMBED_DIM = 256
# QUERY_DIM = 256
# LSTM_STATE_DIM = 256
# DECODER_ATT_HIDDEN_DIM = 50
# POINTER_NET_HIDDEN_DIM = 50
#
# MAX_QUERY_LENGTH = 70
# MAX_EXAMPLE_ACTION_NUM = 100
#
# DECODER_DROPOUT = 0.2
# WORD_DROPOUT = 0
#
# # encoder
# ENCODER_LSTM = 'bilstm'
#
# # decoder
# PARENT_HIDDEN_STATE_FEEDING = True
# PARENT_RULE_FEEDING = True
# NODE_TYPE_FEEDING = True
# TREE_ATTENTION = True
#
# # training
# TRAIN_PATIENCE = 10
# MAX_EPOCH = 50
# BATCH_SIZE = 10
# VALID_PER_MINIBATCH = 4000
# SAVE_PER_MINIBATCH = 4000
#
# # decoding
# BEAM_SIZE = 15
# DECODE_MAX_TIME_STEP = 100

config_info = None
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 3
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 3
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.hs.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/aligned_hs.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
saveto = "../../files/model.django.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 2.5
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.django.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.django.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
saveto = "../../files/model.django.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 2.5
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.django.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.django.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
saveto = "../../files/model.django.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 2.5
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.django.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.django.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
saveto = "../../files/model.django.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 2.5
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.django.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.django.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
saveto = "../../files/model.django.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 2.5
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.django.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.django.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
saveto = "../../files/model.django.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 2.5
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.django.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.django.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
saveto = "../../files/model.django.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 2.5
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.django.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.django.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
saveto = "../../files/model.django.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 2.5
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.django.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.django.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 3
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.hs.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/aligned_hs.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 3
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.hs.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/aligned_hs.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.hs.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/aligned_hs.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.hs.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/aligned_hs.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
saveto = "../../files/model.django.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.django.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.django.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
saveto = "../../files/model.django.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = "../../files/model.django.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.django.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 3
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 3
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 3
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.hs.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/aligned_hs.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 3
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 3
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.hs.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/aligned_hs.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 3
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 3
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.hs.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/aligned_hs.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.hs.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/aligned_hs.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 3
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.hs.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/aligned_hs.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
saveto = "../../files/model.django.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.django.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.django.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
saveto = "../../files/model.django.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.django.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.django.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
saveto = "../../files/model.django.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = "../../files/model.django.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.django.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 3
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.hs.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/aligned_hs.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 3
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 3
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
saveto = "../../files/model.django.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.django.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.django.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
saveto = "../../files/model.django.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.django.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.django.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
saveto = "../../files/model.django.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.django.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
saveto = "../../files/model.django.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.django.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 2101
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 2490
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.django.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 96
random_seed = 181783
data_type = "django"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 222
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/django.cleaned.dataset.freq5.par_info.refact.space_only.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 3
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.hs.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/aligned_hs.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = "../../files/model.hs.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.hs.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/aligned_hs.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
decode_max_time_step = 750
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
optimizer = "adam"
operation = "decode"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 128
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
saveto = "../../files/model.hs.ours.best_acc.npz.decode_results.test.bin"
output_dir = "../../files"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 64
data = "../../files/aligned_hs.bin"
valid_per_batch = 4000
enable_copy = True
retrieval_factor = 3.0
decoder_hidden_dim = 256
max_retrieved_sentences = 3
beam_size = 15
enable_retrieval = True
train_patience = 10
model = "../../files/model.hs.ours.best_acc.npz"
encoder_hidden_dim = 256
decode_max_time_step = 100
max_ngrams = 4
ptrnet_hidden_dim = 50
target_vocab_size = 556
head_nt_constraint = True
mode = "self"
optimizer = "adam"
operation = "evaluate"
use_alignment = False
save_per_batch = 4000
rule_embed_dim = 256
frontier_node_type_feed = True
clip_grad = 0.0
valid_metric = "bleu"
max_epoch = 50
max_query_length = 70
seq2seq_decode_file = None
parent_hidden_state_feed = True
word_embed_dim = 128
source_vocab_size = 351
seq2tree_rareword_map = None
output_dir = "../../files"
is_nbest = False
input = "../../files/model.hs.ours.best_acc.npz.decode_results.test.bin"
type = "test_data"
tree_attention = False
node_num = 57
random_seed = 181783
data_type = "hs"
dropout = 0.2
parent_action_feed = True
encoder = "bilstm"
rule_num = 100
seq2tree_id_file = "test.id.txt"
batch_size = 10
ifttt_test_split = "data/ifff.test_data.gold.id"
attention_hidden_dim = 50
node_embed_dim = 256
data = "../../files/aligned_hs.bin"
seq2seq_ref_file = None
valid_per_batch = 4000
seq2tree_sample_file = "model.sample"
enable_copy = True
retrieval_factor = 0.1
decoder_hidden_dim = 256
max_retrieved_sentences = 10
beam_size = 15
enable_retrieval = False
train_patience = 10
model = None
encoder_hidden_dim = 256
